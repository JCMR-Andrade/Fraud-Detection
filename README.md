# **Fraud Detection â€“ Machine Learning Pipeline**

An end-to-end machine learning pipeline for **fraud detection**, designed to
simulate a production-oriented analytics workflow in a financial context.

Rather than focusing exclusively on model performance, this project emphasizes
**decision-making under risk**, **cost trade-offs**, and **probability reliability** â€”
core challenges in real-world fraud, risk, and compliance systems.

---

## **Objective**

Detect fraudulent transactions using supervised machine learning models,
handling extreme class imbalance and prioritizing evaluation criteria aligned
with **risk management and operational impact**, rather than accuracy alone.

---

## **Dataset**

The project uses a public credit card transaction dataset (UCI / Kaggle),
automatically downloaded by the pipeline to ensure full reproducibility.

The dataset is **already labeled for supervised learning**:

* **Class = 0** â†’ legitimate transaction
* **Class = 1** â†’ fraudulent transaction

Fraud events represent **less than 0.2% of all transactions**, making this a
highly imbalanced classification problem.

The feature set includes:

* **Time**: elapsed time since the first transaction
* **Amount**: transaction value
* **V1 to V28**: anonymized variables obtained via PCA transformation, designed
  to preserve confidentiality while retaining predictive structure

Because labels are already provided, **no heuristic or rule-based fraud detection
is required**. The project focuses on **how models behave once probabilities are
turned into decisions**, not on label discovery.

---

## **Data Preparation Scope**

This project intentionally applies **minimal upfront data treatment**.

Beyond standard scaling and **stratified train/test splitting**, no extensive
feature engineering or exploratory cleaning is performed. This choice is
**deliberate**.

In real-world fraud systems, early stages often include:

* data quality validation
* missing value treatment
* outlier handling
* domain-driven feature engineering
* temporal and behavioral aggregation

Here, those steps are intentionally left out to **isolate the impact of
evaluation strategy, threshold selection, cost modeling, and calibration**.
By starting from a clean, anonymized dataset, the pipeline highlights how
**decision logic â€” not feature engineering â€” can dominate real-world outcomes**.

---

## **Approach and Techniques**

The pipeline was built incrementally, following a **decision-oriented approach**
rather than a model-centric one.

It includes:

* **Logistic Regression** as a transparent and interpretable baseline
* **Random Forest** as a non-linear ensemble model
* Explicit comparison against a **naive baseline**
* **Precisionâ€“Recall** as the primary evaluation framework
* **Threshold tuning** to enforce a minimum fraud recall constraint
* **Cost simulation** based on false positives and false negatives
* **Probability calibration** to align scores with observed risk
* **Feature importance analysis** for interpretability

This structure mirrors how fraud models are typically evaluated and deployed in
production environments.

---

## **Tech Stack**

* **Python**
* **Pandas / NumPy**
* **Scikit-learn**
* **Matplotlib / Seaborn**
* **Structured logging (TXT + JSON)**

---

## **Pipeline Overview**

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Download    â”‚
â”‚ (Public Dataset)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Preparation   â”‚
â”‚  â€¢ Scaling         â”‚
â”‚  â€¢ Stratified Splitâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model Training     â”‚
â”‚  â€¢ Logistic Reg.   â”‚
â”‚  â€¢ Random Forest   â”‚
â”‚  â€¢ Naive Baseline  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Evaluation         â”‚
â”‚  â€¢ PR Curve        â”‚
â”‚  â€¢ ROC             â”‚
â”‚  â€¢ Confusion Matrixâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Threshold Tuning   â”‚
â”‚  â€¢ Recall target   â”‚
â”‚  â€¢ Cost-aware      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Calibration        â”‚
â”‚  â€¢ Isotonic        â”‚
â”‚  â€¢ Risk alignment  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Decision Outputs   â”‚
â”‚  â€¢ Cost comparison â”‚
â”‚  â€¢ Logs & reports  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## **Final Model Decision**

After evaluating all models against a naive baseline, the **Random Forest
classifier** was selected as the final solution.

This decision was driven by:

* Superior behavior on the **Precisionâ€“Recall curve**
* A more favorable redistribution of **false negatives and false positives**
* The **lowest total expected cost** under simulated risk assumptions

Instead of relying on a default probability threshold, the modelâ€™s threshold was
**explicitly tuned to enforce a minimum fraud recall**, aligning model behavior
with operational risk tolerance.

> In highly imbalanced problems such as fraud detection,
> **threshold choice and cost modeling often matter more than algorithm choice**.

---

## **Calibration and Risk Alignment**

After threshold tuning, **probability calibration** was applied to assess whether
model scores reflected real-world fraud risk.

Calibration does not aim to improve ranking metrics such as ROC or Average
Precision. Instead, it adjusts the **probability scale itself**, making scores
more reliable for operational decisions.

Key observations:

* **Logistic Regression** showed limited gains from calibration
* **Random Forest** exhibited overconfidence prior to calibration, which was
  mitigated using **isotonic regression**
* The **naive baseline** provided no meaningful signal for calibration

As a result, thresholds became more stable and cost simulations more reliable.

## ðŸš€ How to Run
```bash
pip install -r requirements.txt
python fraud_detection_pipeline.py
```
---

## **Related Article**

A detailed walkthrough of the project â€” from the initial idea to the final
risk-based decision â€” will be published on Medium:

**https://medium.com/@julio.pimp/por-que-accuracy-falha-em-projetos-reais-de-detec%C3%A7%C3%A3o-de-fraudes-ae405622aa2c**

